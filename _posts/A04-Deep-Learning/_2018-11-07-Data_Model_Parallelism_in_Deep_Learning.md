---
layout: post
title:  深度学习中GPU的使用
date:   2018-11-06 09:00:00 +0800
tags: Deep-Learning AI GPU
categories: Deep-Learning AI
typora-root-url: ..\..
---

{% include lib/mathjax.html%}

数据正以前所未有的规模生成。大规模互联网公司每天都会生成数以 TB 计的数据，这些数据都需要得到有效的分析以提取出有意义的见解 [1]。新兴的深度学习是一种用于执行这种分析的强大工具，这些算法在视觉 [2]、语言 [3] 和智能推理 [4] 领域的复杂任务上创造着当前最佳。不幸的是，这些算法需要大量数据才能有效地完成训练，而这又会耗费大量时间。第一个在 ImageNet 分类任务上取得当前最佳结果的深度学习算法在单个 GPU 上训练足足耗费了一周时间

在大数据时代，解决存储和算力的方法是Scale out，在AI时代，Scale out也一定是发展趋势，并且大数据分析任务和AI/ML任务会共享处理设备（由于AI/ML迭代收敛和容错的特征，这两种任务未来不太可能使用统一平台）。

#### **分布式训练概述**

##### 为什么要分布式训练深度学习模型 

1. 增加训练的吞吐量，缩短模型训练时间；
2. 第二个原因是针对大模型训练，大模型通常在一个设备中放不下。 

##### **分布式训练的方式**

神经网络的分布式训练可以通过两种方式实现：数据并行化（data parallelism）和模型并行化（model parallelism）以及两者结合。

![](/assets/imgs/A04/model-data-parallelism-1.png)

**数据并行化**

目标是将数据集均等地分配到系统的各个节点（node），其中每个节点都有该神经网络的一个副本及其本地的权重。每个节点都会处理该数据集的一个不同子集并更新其本地权重集。这些本地权重会在整个集群中共享，从而通过一个累积算法计算出一个新的全局权重集。这些全局权重又会被分配至所有节点，然后节点会在此基础上处理下一批数据。

**模型并行化**

通过将该模型的架构切分到不同的节点上来实现训练的分布化。

AlexNet 是使用模型并行化的最早期模型之一，其方法是将网络分摊到 2 个 GPU 上以便模型能放入内存中。

当模型架构过大以至于无法放入单台机器且该模型的某些部件可以并行化时，才能应用模型并行化。模型并行化可用在某些模型中，比如目标检测网络，这种模型的绘制边界框和类别预测部分是相互独立的。一般而言，大多数网络只可以分配到 2 个 GPU 上，这限制了可实现的可扩展数量。

**模型并行化与数据并行化结合**

假设有一个多GPU集群系统。我们可以在同一台机器上采用模型并行化（在GPU之间切分模型），在机器之间采用数据并行化。 

![](/assets/imgs/A04/model-data-parallelism-2.png)

#### **分布式训练框架的组件**

##### 分布式训练算法

一个常用于分布式模型训练的算法是随机梯度下降(Stochastic Gradient Descent, SGD)，其原理参见[随机梯度下降原理]()。针对 SGD 提及的原则可以轻松地移植给其它常用的优化算法，比如 Adam、RMSProp等等。分布式 SGD 可以大致分成两类变体：异步 SGD 和同步 SGD。

**同步 SGD**

网络中的节点紧密地耦合。

优缺点：

**异步 SGD** 

通过降低节点之间的依赖程度而去除不同节点之间的耦合。尽管这种解耦能实现更大的并行化，但却不幸具有副作用，即稳定性和准确性要稍微差一些。

优缺点：



**batch size**

在大 mini-batch 上训练时，模型能以更大的步幅到达局部最小值，由此能够加快优化过程的速度。

但是在实践中，使用大批量会导致发散问题(divergence problems)或泛化差距(generalization gap)，即网络的测试准确度有时会低于在更小批量上训练的模型。最近的一些研究通过与批量大小成比例地调整学习率而实现了在大批量上的训练。

实验发现，增加批量大小就相当于降低学习率 [11]，而使用大批量进行训练还有一个额外的好处，即在训练中所要更新的总参数更少。

##### 节点之间的通信

高效通信、容错

高效的网络结构：在最优的时间内完成互连节点之间的梯度传输。

- All Reduce
- Ring All Reduce
- 递归减半或倍增（Recursive Halfing/Doubling）
- Binary Blocks 算法

最少需要传输的数据量

- 梯度压缩
- 混合精度训练
- 使用周期性的学习率能将实现网络收敛所需的 epoch 数量降低 10 倍





#### TensorFlow分布式计算原理

TensorFlow的并行计算形式分为单台服务器内多块GPU（简称“单机多卡”）并行和若干台多卡服务器（简称“多机多卡”）并行。

##### **单机多卡上的图内（in-graph）并行**

TensorFlow只使用1个CPU核心，即CPU0。绝大部分计算量均由GPU完成。

为了使所有GPU同时工作，需要将计算图划分成多个子图，每个子图对应一块GPU。CPU将各个子图上的计算任务指派给其对应的GPU，全程具中调度，调度过程大致如下：

- 计算图初始化：组装出计算图，给变量赋初值、生成随机数等。
- 划分子图：原则是子图之间的相互依赖尽量弱，各子图上包含的计算量尽量均衡。【实现原理是什么？】
- 作业指派：CPU将子图中的操作指派给对应GPU进行运算，少量不能在GPU上完成的图节点由CPU完成；指派步骤包括先将所需数据制到GPU上，再启动CUDA kernel来在这些数据上计算。
- 数据汇总交换：GPU使用自己显存上的数据更新图节点的值；CPU将所有子图的最新值从显存复制回CPU内存，汇总得到整个计算图的新值。然后将新数值分发给各个GPU。
- 各个GPU使用新值进下一轮计算。

##### 多机多卡上的图间（between-graph）并行

图内并行只能使用一台服务器，GPU数量有限，导致大规模训任务练动辄消耗几天几周的时间，效率低下。

但是，如果将计算图剖分成多个子图，将一个子图指派给一台服务器，那么将面临几个难题：

- 计算图不够大，难以划分出足够多的子图。例如采用100台服务器，而深度神经网络只有1个输入层（784个神经元）、1个隐层（30个神经元）、1个输出层（10个神经元），这样的计算图几乎是不可剖分的。如果按神经元的层剖分，就是3级流水线，效率不高不说，关键是只能指派给3台服务器。
- 网络通信压力大：子图之间多多少少都有一些依赖关系，即使强行剖分成N个子图，子图之间的数据交换将带来巨大的网络压力，影响整体计算效率。

图内并行时不存在这两个难题，原因有两个：

- 需要子图数量少（通常2/4个，多的8/16个），从而剖分容易；

- 在服务器内存中汇总交换数据，PCIe的带宽和延时性能都比服务器之间的网络性能好很多。

**数据并行**



**TensorFlow分布式训练的网络瓶颈分析**



#### 如何配置单机多卡训练模型

#### 如何配置多级多卡训练模型

参数共享分发

训练数据是如何分发的？

由master把数据分片分别分发到不同worker，还是全量复制到不同worker？