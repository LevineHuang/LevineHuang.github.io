---
typora-root-url: ../../../LevineHuang.github.io
typora-copy-images-to: ../../assets/imgs/A06
---

近年来，深度学习方法极大的推动了自然语言处理领域的发展。几乎在所有的 NLP 任务上我们都能看到深度学习技术的应用，并且在很多的任务上，深度学习方法的表现大大超过了传统方法。可以说，深度学习方法给 NLP 带来了一场重要的变革。

自然语言处理的目标是什么，任务是什么，主要的方法大概有哪些。

## NLP基本概念

Natural Language Processing( NLP)，主要是指借助于计算技术，对人类的自然语言进行分析、理解，还有生成的过程。

应用场景

- 对话机器人（chatbot），如 AI 音箱
- 机器翻译

## **NLP任务的类型**

### 从语言要素角度分类

从语言要素角度，可以分类以下任务：

#### 词法分析

##### 分词任务

##### 形态分析

主要针对形态丰富拉丁语系的语言。给定一个词，把里面的词干、词缀、词根等拆分出来，然后做一些形态还原、形态切分任务，然后给其它任务提供一个更好的输入。

##### 先词性标注

##### 先拼写校正

编辑器自动纠错的功能

##### 关键词搜索

##### 同义词发现

##### 新词发现

发掘发现文本中的一些新词最新的一些词，比如说“活久见“、”十动然拒“、”十动然揍”这种网络热词。

#### **句法分析（Sentence Analysis）**

对自然语言进行句子层面的分析，包括句法分析（Syntactic Parsing）和其它句子级别的分析任务。

##### 组块分析（Chunking）

标出句子中的短语块，例如名词短语（NP）、动词短语（VP）等 。

##### 超级标签标注（Super Tagging）

给每个句子中的每个词标注上超级标签。超级标签是句法树与该词相关的树形结构。

##### 成分句法分析（Constituency Parsing）

分析句子的成分，给出一颗由终结符和非终结符构成的成分句法树。

##### 依存句法分析（Dependency Parsing）

分析句子中词之间的依存关系，给一颗由词语依存关系构成的依存句法树。

##### 语言模型任务

训练设计一个模型来对语句合理的程度（流畅度）进行一个打分。

##### 语种识别任务

识别一段文本成到底是用哪一个语言书写的。

##### 句子边界检测

对于 一些语言来说，句子之间是没有明显边界的，所以做句子层面的分析之前，首先要对它进行句子边界的检测，比如泰语。

#### 语义分析（Semantic Analysis）

##### 词义消歧

##### 语义角色标注

标出句子里语义决策动作的发起者，受到动作影响的角色等等。比如 “A 打了 B”，那么 A 就是一个施事， B 就是一个受事，中间就是一个“打”的动作。

##### 抽象语义表示（Abstract Semantic Parsing，AMR）

词汇、句子、段落的一个向量化表示，即Word/Sentence/Paragraph Vector，包括研究向量化的方法和向量性质以及应用。

#### **信息抽取（Information Extraction）**

##### 命名实体识别

##### 实体消歧

##### 术语抽取

##### 共指消解、名词消解

##### 关系抽取任务

确定文本当中两个实体之间的关系，比如说谁生了谁，两个实体一个是生一个是被生。

##### 事件抽取任务

抽取出时间、地点、人物、发生的事件等等，这是更结构化的信息抽取。

##### 情感分析

##### 意图识别

是对话系统中一个比较重要的模块，是要分析就是说用户跟对话机器人说话的时候这句话的目的是什么

##### 槽位填充

和意图识别搭配起来使用。意图识别出来后，意图要有具体的信息，比如意图是让机器人帮忙预定明天早上从北京到上海飞的一张机票，意图识别出来是定机票，那么要抽取一些信息的槽位，比如时间是“明天早上”，出发点是“北京”，目的地是“上海”，这样才能配合起来做后面的一些程序性的工作。

#### 篇章分析

篇章分析的最终目标是从整体上理解篇章，最重要的任务是分析篇章结构。篇章结构包括：语义结构，话题结构，指代结构等。

### 从任务类型角度分类

从任务的类型角度，可以分为以下四类：

一类是序列标注，这是最典型的 NLP 任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。第三类任务是句子关系判断，比如 Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。



#### 序列标注

特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。例如：分词、词性标注，命名实体识别，语义角色标注等

#### 分类任务

特点是不管输入文本有多长，总体给出一个分类类别。例如：文本分类、情感分类等。

#### 句子关系判断

特点是给定两个句子，模型判断出两个句子是否具备某种语义关系。例如：Entailment、QA、自然语言推理等。

#### 生成式任务

特点是输入文本内容后，需要自主生成另外一段文字。例如：机器翻译、文本摘要、写诗造句、看图说话等

## NLP任务的特点

## NLP中数据预处理方法

### 英文文本数据预处理

### 中文文本数据预处理

## NLP中词嵌入方法

## NLP中特征提取方法

### NLP任务输入的特点

NLP 的输入往往是一句话或者一篇文章，所以它有几个特点：首先，输入是个一维线性序列，这个好理解；其次，输入是不定长的，有的长有的短，而这点其实对于模型处理起来也会增加一些小麻烦；再次，单词或者子句的相对位置关系很重要，两个单词位置互换可能导致完全不同的意思。句子中的长距离特征对于理解语义也非常关键。



Bert 这种两阶段的模式（预训练+Finetuning）必将成为 NLP 领域研究和工业应用的流行方法；第二个是从 NLP 领域的特征抽取器角度来说，Transformer 会逐步取代 RNN 成为最主流的的特征抽取器。



一个特征抽取器是否适配问题领域的特点，有时候决定了它的成败，而很多模型改进的方向，其实就是改造得使得它更匹配领域问题的特性。

RNN 本身的序列依赖结构对于大规模并行计算来说相当之不友好。



## NLP中的深度学习模型

### RNN在NLP中的运用

存在问题：

1. 并行计算能力比较差

改进方法：

为什么 RNN 并行计算能力比较差？是什么原因造成的？

如何改造 RNN 使其具备并行计算能力？

有两个大的思路来改进：一种是仍然保留任意连续时间步（T-1 到 T 时刻）之间的隐层连接；而另外一种是部分地打断连续时间步（T-1 到 T 时刻）之间的隐层连接。

多神经元？那么在隐层神经元之间并行计算行

把隐层之间的神经元依赖由全连接改成了哈达马乘积，这样 T 时刻隐层单元本来对 T-1 时刻所有隐层单元的依赖，改成了只是对 T-1 时刻对应单元的依赖，于是可以在隐层单元之间进行并行计算，

SRNN 速度比 GRU 模型快 5 到 15 倍，嗯，效果不错，但是跟对比模型 DC-CNN 模型速度比较起来，比 CNN 模型仍然平均慢了大约 3 倍。



怀旧版 CNN 存在哪些问题，然后看看我们的 NLP 专家们是如何改造 CNN，一直改到目前看上去还算效果不错的现代版本 CNN 的。

### CNN在NLP中的运用

卷积神经网络（Convolution Neural Network, CNN）在数字图像处理领域取得了巨大的成功，从而掀起了深度学习在自然语言处理领域（Natural Language Processing, NLP）的狂潮。

自从2013年Google关于**词嵌入**(**word embedding**)的工作出来之后，利用word embedding作为模型输入变成了一种常用的文本预处理。与此同时，**CNN**(卷积神经网络)在计算机视觉(CV,computer vision)取得了非常好的成绩，这很自然地让人想到可以将卷积操作应用在词嵌入矩阵上，自动提取特征来处理NLP的任务。

在NLP中CNN的输入可以是什么？

如果输入是词向量，每一行代表一个词，那么如何解决不同的文本长度不统一的问题？

CNN 捕获到的是什么特征呢？

NLP中CNN的常用超参数设置都有哪些？

NLP中CNN存在什么问题？如何解决呢？

接下来CNN在NLP的研究还可以从哪个方向进行？



https://www.jianshu.com/p/1267072ee8f8

https://galaxychen.github.io/2018/03/03/CNN%E5%9C%A8NLP%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/

http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/





关键在于卷积核覆盖的那个滑动窗口，CNN 能捕获到的特征基本都体现在这个滑动窗口里

捕获到的是单词的 k-gram 片段信息，这些 k-gram 片段就是 CNN 捕获到的特征，k 的大小决定了能捕获多远距离的特征。

只有一个卷积层带来的问题是：对于远距离特征，单层 CNN 是无法捕获到的，如果滑动窗口 k 最大为 2，而如果有个远距离特征距离是 5，那么无论上多少个卷积核，都无法覆盖到长度为 5 的距离的输入，所以它是无法捕获长距离特征的。

跳着覆盖呀，是吧？这就是 Dilated 卷积的基本思想，确实也是一种解决方法。

Dilated CNN 偏技巧一些，而且叠加卷积层时超参如何设置有些学问，因为连续跳接可能会错过一些特征组合，所以需要精心调节参数搭配，保证所有可能组合都被覆盖到。相对而言，把 CNN 作深是主流发展方向

目前看来，还是深层网络参数优化手段不足导致的这个问题，而不是层深没有用。后来 Resnet 等图像领域的新技术出现后，很自然地，人们会考虑把 Skip Connection 及各种 Norm 等参数优化技术引入，这才能慢慢把 CNN 的网络深度做起来。

卷积层后面立即接上 Pooling 层的话，Max Pooling 的操作逻辑是：从一个卷积核获得的特征向量里只选中并保留最强的那一个特征，所以到了 Pooling 层，位置信息就被扔掉了，这在 NLP 里其实是有信息损失的。所以在 NLP 领域里，目前 CNN 的一个发展趋势是抛弃 Pooling 层，靠全卷积层来叠加网络深度，这背后是有原因的（当然图像领域也是这个趋

Kim 2014 CNN

Dilated CNN

加深

抛弃Pooling层，保留位置信息

### Transformer

### Transformer XL

长程依赖现象在序列数据中非常常见，我们必须理解其含义，这样才能处理很多现实任务。例如，为了正确理解一篇文章，读者有时需要返回前文，参考在几千字之前出现的一个词或句子。

Transformer允许在数据单元之间建立直接联系，从而有可能更好地捕捉长期依赖关系。然而，在语言建模中，Transformer 目前使用固定长度的上下文来实现，即将一个长文本序列截成多个包含数百个字符的长度固定的句段，然后单独处理每个句段。 这引入了两个关键限制：

- 算法无法对超过固定长度的依赖关系建模
- 句段通常不遵循句子边界，从而造成上下文碎片化，导致优化效率低下。在长程依赖性不成其为问题的短序列中，这种做法尤其令人烦扰

![img](/assets/imgs/A06/0*xKYSYsJkLjAZAoSr.png)

为了解决这些限制，推出了 Transformer-XL 架构，可以实现超出固定长度上下文的自然语言理解。Transformer-XL 采用了两项技术：句段层级的循环机制和相对位置编码方案。

#### 句段层级的循环机制

在训练期间，系统为前一个句段计算的表征会被修正并缓存，当模型处理下一个新句段时，即可将其作为扩展上下文重新使用。上下文信息现在可以跨句段边界流动，因此，这一额外联系将可能的最大依赖关系长度扩大了 N 倍，其中 N 是网络深度。此外，这一递归机制还可以解决上下文碎片化问题，从而为新句段前面的tokens提供必要的上下文。

![img](/assets/imgs/A06/0*u-S6KtrNpYFtOl0k.png)

#### 相对位置编码方案

提出了一个新的相对位置编码方案，以使递归机制的应用成为可能。此外，与其他相对位置编码方案不同，方案使用了具备**可习得转换**的固定嵌入，而**非可习得嵌入**，因此该方案在测试时可以更广泛地适用于较长的序列。当我们将这两种方法结合使用时，Transformer-XL 在评估期间拥有的有效上下文会比 Vanilla Transformer 模型长得多。

另外，Transformer-XL 可以在不进行重新计算的情况下同时处理新句段中的所有元素，进而显著提升速度（在下文讨论）。



### BERT

## NLP中的语言学

#### 从语言学角度看词嵌入模型

https://mp.weixin.qq.com/s/_7o6_2dWkx4yUsYu1_Bj7g

## NLP模型预训练



## 深度学习在自然语言处理中的应用

### 字符识别

字符识别系统具有许多应用，如收据字符识别，发票字符识别，检查字符识别，合法开票凭证字符识别等。文章《Character Recognition Using Neural Network》提出了一种具有85％精度的手写字符的方法。

### 拼写检查

大多数文本编辑器可以让用户检查其文本是否包含拼写错误。神经网络现在也被并入拼写检查工具中。

在《Personalized Spell Checking using Neural Networks》，作者提出了一种用于检测拼写错误的单词的新系统。这个系统通过打字员做出的具体修正的数据进行模型训练。它揭示了传统拼写检查方法的许多缺点。

### 信息抽取

信息抽取的主要任务是从非结构化文档自动导出结构化信息。该任务包括许多子任务，如命名实体识别，一致性解析，关系抽取，术语抽取等。

### 命名实体识别（NER）

命名实体识别（NER）的主要任务是将诸如Guido van Rossum，Microsoft，London等的命名实体分类为人员，组织，地点，时间，日期等预定类别。许多NER系统已经创建，其中最好系统采用的是神经网络。

在《Neural Architectures for Named Entity Recognition》文章中，提出了两种用于NER模型。这些模型采用有监督的语料学习字符的表示，或者从无标记的语料库中学习无监督的词汇表达[4]。使用英语，荷兰语，德语和西班牙语等不同数据集，如CoNLL-2002和CoNLL-2003进行了大量测试。该小组最终得出结论，如果没有任何特定语言的知识或资源（如地名词典），他们的模型在NER中取得最好的成绩。

### 词性标注

词性标注（POS）具有许多应用，包括文本解析，文本语音转换，信息抽取等。在《Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network》工作中，提出了一个采用RNN进行词性标注的系统。该模型采用《Wall Street Journal data from Penn Treebank III》数据集进行了测试，并获得了97.40％的标记准确性。

### 文本分类

文本分类是许多应用程序中的重要组成部分，例如**网络搜索，信息过滤，语言识别，可读性评估和情感分析**。神经网络主要用于这些任务。

Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao在论文《Recurrent Convolutional Neural Networks for Text Classification》中，提出了一种用于文本分类的循环卷积神经网络，该模型没有人为设计的特征。该团队在四个数据集测试了他们模型的效果，四个数据集包括：20Newsgroup（有四类，计算机，政治，娱乐和宗教），复旦大学集（中国的文档分类集合，包括20类，如艺术，教育和能源），ACL选集网（有五种语言：英文，日文，德文，中文和法文）和Sentiment Treebank数据集（包含非常负面，负面，中性，正面和非常正面的标签的数据集）。测试后，将模型与现有的文本分类方法进行比较，如Bag of Words，Bigrams + LR，SVM，LDA，Tree Kernels，RecursiveNN和CNN。最后发现，在所有四个数据集中，神经网络方法优于传统方法，他们所提出的模型效果优于CNN和循环神经网络。

### 语义分析和问题回答

问题回答系统可以自动回答通过自然语言描述的不同类型的问题，包括定义问题，传记问题，多语言问题等。神经网络可以用于开发高性能的问答系统。

在《Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base》文章中，Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao描述了基于知识库来开发问答语义解析系统的框架框架。作者说他们的方法早期使用知识库来修剪搜索空间，从而简化了语义匹配问题。他们还应用高级实体链接系统和一个用于匹配问题和预测序列的深卷积神经网络模型。该模型在WebQuestions数据集上进行了测试，其性能优于以前的方法。

### 释义检测

释义检测确定两个句子是否具有相同的含义。这个任务对于问答系统尤其重要，因为同样的问题可以有多种描述方式。

《Detecting Semantically Equivalent Questions in Online User Forums》文中提出了一种采用卷积神经网络来识别语义等效性问题的方法。实验使用Ask Ubuntu社区问答（Q＆A）站点和Meta Stack Exchange数据来进行网络训练。已经表明，所提出的CNN模型取得了很高的精度，特别是采用领域相关的数据来预训练Word Embedding之后。作者将他们的模型的性能与支持向量机和重复检测方法等传统方法进行了比较。他们表示，他们的CNN模型大大优于传统的baseline。

《 Paraphrase Detection Using Recursive Autoencoder》文中提出了使用递归自动编码器的进行释义检测的一种新型的递归自动编码器架构。它使用递归神经网络学习短语表示。这些表示是在n维语义空间中的向量，其中具有相似含义的短语彼此接近[8]。为了评估系统，使用Microsoft Research Paraphrase语料库和英语Gigaword语料库。该模型与三个baseline进行比较，优于它们。

### 语言生成和多文档总结

自然语言生成有许多应用，如自动撰写报告，基于零售销售数据分析生成文本，总结电子病历，从天气数据生成文字天气预报，甚至生成笑话。

研究人员在最近的一篇论文《 Natural Language Generation, Paraphrasing and Summarization of User Reviews with Recurrent Neural Networks》中，描述了基于循环神经网络（RNN）模型，能够生成新句子和文档摘要的。该论文描述和评估了俄罗斯语820,000个消费者的评论数据库。网络的设计允许用户控制生成的句子的含义。通过选择句子级特征向量，可以指示网络学习，例如，“在大约十个字中说出一个关于屏幕和音质的东西”。语言生成的能力可以生成具有不错质量的，多个用户评论的抽象摘要。通常，总结报告使用户可以快速获取大型文档集中的主要信息。

### 机器翻译

机器翻译软件在世界各地使用，尽管有限制。在某些领域，翻译质量不好。为了改进结果，研究人员尝试不同的技术和模型，包括神经网络方法。《Neural-based Machine Translation for Medical Text Domain》研究的目的是检查不同训练方法对用于，采用医学数据的，波兰语-英语机器翻译系统的影响。采用The European Medicines Agency parallel text corpus来训练基于神经网络和统计机器翻译系统。证明了神经网络需要较少的训练和维护资源。另外，神经网络通常用相似语境中出现的单词来替代其他单词。

### 语音识别

语音识别应用于诸如家庭自动化，移动电话，虚拟辅助，免提计算，视频游戏等诸多领域。神经网络在这一领域得到广泛应用。

在《Convolutional Neural Networks for Speech Recognition》文章中，科学家以新颖的方式解释了如何将CNN应用于语音识别，使CNN的结构直接适应了一些类型的语音变化，如变化的语速。在TIMIT手机识别和大词汇语音搜索任务中使用。

#### 语音机器人

智能客服，设备控制





张金超博士，微信模式识别中心的高级研究员，毕业于中国科学院计算技术研究所，研究方向是自然语言处理、深度学习，以及对话系统。



## 参考文献

1. [10 Applications of Artificial Neural Networks in Natural Language Processing](https://medium.com/@datamonsters/artificial-neural-networks-in-natural-language-processing-bcf62aa9151a)

2. 统计自然语言处理（第2版）,宗成庆，中国科学院自动化所研究员、博士生导师。主要从事自然语言处理、机器翻译和文本分类等相关技术的研究和教学工作。
3. https://cloud.tencent.com/developer/article/1079600
4. https://www.leiphone.com/news/201712/goDKAGGL7qvtfCQ8.html
5. https://www.jiqizhixin.com/articles/2017-08-21-5
6. https://www.leiphone.com/news/201804/I5zKpCHQq5oZJm4p.html
7. https://www.tinymind.cn/articles/1207

