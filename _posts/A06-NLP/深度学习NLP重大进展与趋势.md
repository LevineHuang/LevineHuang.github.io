## 深度学习NLP重大进展与趋势



### 自然语言中的多任务学习

多任务学习是将多个任务一起学习，充分挖掘多个任务之间的相关性，来提高每个任务的模型准确率，从而可以减少每个任务对训练数据量的需求。

http://www.mooc.ai/open/course/574

### 自然语言中的迁移学习

希望能像图像领域那样，通过大量数据来预训练一个大型的神经网络，然后用它来对文本提取特征去做后续的任务，以期望能得到更好的效果。

#### ELMo

由AllenAI提出。

论文：https://arxiv.org/pdf/1802.05365.pdf

#### CMRC2018阅读理解比赛

追一科技的参赛方案。

将英文的字符级别的编码改进为笔画级别的编码，同时结合原有的词级别编码一起通过双层LSTM变换来进行语言模型预训练。经过实验验证，最后选择了512维的词级别ELMo向量进行后续任务。

#### ULMFiT

由FastAI提出。在微调时对每一层设置不同的学习率。

论文:https://arxiv.org/abs/1801.06146

#### Transfomer

舍弃了RNN的循环式网络结构，完全基于注意力机制来对一段文本进行建模。

Transformer所使用的注意力机制的核心思想是去计算一句话中的每个词对于这句话中所有词的相互关系，然后认为这些词与词之间的相互关系在一定程度上反应了这句话中不同词之间的关联性以及重要程度。因此再利用这些相互关系来调整每个词的重要性（权重）就可以获得每个词新的表达。这个新的表征不但蕴含了该词本身，还蕴含了其他词与这个词的关系，因此和单纯的词向量相比是一个更加全局的表达。

Transformer通过对输入的文本不断进行这样的注意力机制层和普通的非线性层交叠来得到最终的文本表达。

论文：https://arxiv.org/abs/1706.03762

#### GPT

由FastAI提出。利用了Transformer的结构来进行单向语言模型的训练。

预训练的语言模型是在百度15亿词文本的语料上进行的，模型参数选择了12层，12head的Transformer结构。然后采用此模型直接在子任务上微调来进行后续任务。

论文：https://blog.openai.com/language-unsupervised/

#### BERT 

由Google提出。在OpenAI的GPT的基础上对预训练的目标进行了修改，通过在33亿文本的语料上训练语言模型，再分别在不同的下游任务上微调，这样的模型在11项NLP任务均得到了目前为止最好的结果，并且有一些结果相比此前的最佳成绩得到了幅度不小的提升。

论文：https://arxiv.org/abs/1810.04805