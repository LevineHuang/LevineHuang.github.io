## 深度学习NLP重大进展与趋势



### 自然语言中的多任务学习

多任务学习是将多个任务一起学习，充分挖掘多个任务之间的相关性，来提高每个任务的模型准确率，从而可以减少每个任务对训练数据量的需求。

http://www.mooc.ai/open/course/574

### 自然语言中的迁移学习

希望能像图像领域那样，通过大量数据来预训练一个大型的神经网络，然后用它来对文本提取特征去做后续的任务，以期望能得到更好的效果。

#### ELMo

由AllenAI提出。

论文：https://arxiv.org/pdf/1802.05365.pdf

#### CMRC2018阅读理解比赛

追一科技的参赛方案。

将英文的字符级别的编码改进为笔画级别的编码，同时结合原有的词级别编码一起通过双层LSTM变换来进行语言模型预训练。经过实验验证，最后选择了512维的词级别ELMo向量进行后续任务。

#### ULMFiT

由FastAI提出。在微调时对每一层设置不同的学习率。

论文:https://arxiv.org/abs/1801.06146

#### Transfomer

舍弃了RNN的循环式网络结构，完全基于注意力机制来对一段文本进行建模。

Transformer所使用的注意力机制的核心思想是去计算一句话中的每个词对于这句话中所有词的相互关系，然后认为这些词与词之间的相互关系在一定程度上反应了这句话中不同词之间的关联性以及重要程度。因此再利用这些相互关系来调整每个词的重要性（权重）就可以获得每个词新的表达。这个新的表征不但蕴含了该词本身，还蕴含了其他词与这个词的关系，因此和单纯的词向量相比是一个更加全局的表达。

Transformer通过对输入的文本不断进行这样的注意力机制层和普通的非线性层交叠来得到最终的文本表达。

论文：https://arxiv.org/abs/1706.03762

#### GPT

由FastAI提出。利用了Transformer的结构来进行单向语言模型的训练。

预训练的语言模型是在百度15亿词文本的语料上进行的，模型参数选择了12层，12head的Transformer结构。然后采用此模型直接在子任务上微调来进行后续任务。

论文：https://blog.openai.com/language-unsupervised/

#### BERT 

由Google提出。在OpenAI的GPT的基础上对预训练的目标进行了修改，通过在33亿文本的语料上训练语言模型，再分别在不同的下游任务上微调，这样的模型在11项NLP任务均得到了目前为止最好的结果，并且有一些结果相比此前的最佳成绩得到了幅度不小的提升。

论文：https://arxiv.org/abs/1810.04805





## NLP发展趋势

### 对话MSRA副院长周明：回望过去，展望未来，NLP有哪些发展趋势？

从宏观层次和技术层面探讨 NLP 的研究进展及未来发展趋势。

#### 过去一年 NLP 领域有哪些新的进展

第一，神经网络深入到 NLP 各个领域之中，由此带来的崭新的神经 NLP 的建模、学习和推理方法

第二，以 BERT 为代表的一系列预训练模型得到了广泛应用，体现了大规模语言数据所蕴含的普遍语言规律和知识与具体应用场景巧妙结合的潜力；

第三，低资源的 NLP 任务获得了广泛重视并得到了很好的发展。

「NLP 进入了黄金的十年」。这是因为未来国民经济发展和人工智能对 NLP 带来的庞大的需求，大规模的各类数据可供模型训练，以神经网络 NLP 为代表的各种新方法将一步步提升建模水平，各种评测和各种开放平台推动 NLP 研究和推广的能力，日益繁荣的 AI 和 NLP 领域促进专门人才的培养等等。所以，未来十年是非常值得期待的。

#### 2019 年哪些研究会有更大的研究潜力

第一，预训练模型。包括怎么训练一个更好的预训练模型，包括怎么把预训练模型更好地应用在某一项具体任务里面。

第二，低资源 NLP 任务的研究。无语料或者小语料的场合。把其他语言、任务或者开放领域的模型巧妙嫁接或者借用到新的语言、任务或者领域中来，

第三，如何把知识和常识建立起来，如何巧妙地加入到模型里头，然后如何评测知识和常识所带来的效果。

《NLP 将迎来黄金十年》一文中指出，NLP 将向四个方面倾斜，分别是：

1）将知识和常识引入到目前基于数据的学习系统中；

2）低资源的 NLP 任务的学习方法；

3）上下文建模、多轮语义理解；

4）基于语义分析、知识和常识的可解释 NLP。

#### 研究方法论

方法论与应用实践结合，相得益彰。

一个是方法，一个是应用，让它俩不停的迭代。应用给很多方法提供了挑战，然后很多方法给应用提供了新的一些想法，两者可以相得益彰。

modeling、learning、reasoning。三个 ing 的东西实际上是自然语言作为一个学科，建立其背后最重要的技术体系和理论体系。在理论体系支撑下，希望最后形成一个技术体系。

多模态融合

由于神经网络的进展，使得多模态（语言、文字、图像、视频）的编码和解码可以在同一个框架下统一进行了

现在还没有一个特别好的方法把两者融合起来，也没有把语言学知识或领域知识给予充分的表达，以体现它的能力。这方面在研究上还有欠缺，但恰恰也是未来的一个研究焦点。

数据足够充分，可以倾向于端对端的自动学习，数据不够充分，且具备可用的知识和规则，则没有理由不去利用知识和规则，以快速建立系统。当系统运行起来，需要考虑不停地追加数据、知识和用户的反馈以改进系统。所以一个实用的 NLP 系统，是数据、知识和用户一起磨炼而成的。





## 参考文献

