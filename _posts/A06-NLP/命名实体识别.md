---
typora-root-url: ../../../LevineHuang.github.io
---



## **命名实体识别（Named Entity Recognition）**

​      命名实体识别（Named Entity Recognition, NER）是 NLP 里的一项很基础的任务，就是指从文本中识别出命名性指称项，为关系抽取等任务做铺垫。狭义上，是识别出人名、地名和组织机构名这三类命名实体（时间、货币名称等构成规律明显的实体类型可以用正则等方式识别）。当然，在特定领域中，会相应地定义领域内的各种实体类型

汉语作为象形文字，相比于英文等拼音文字来说，针对中文的NER任务来说往往要更有挑战性，下面列举几点：

​      (1) 中文文本里不像英文那样有空格作为词语的界限标志，而且“词”在中文里本来就是一个很模糊的概念，中文也不具备英文中的字母大小写等形态指示

​      (2) 中文的用字灵活多变，有些词语在脱离上下文语境的情况下无法判断是否是命名实体，而且就算是命名实体，当其处在不同的上下文语境下也可能是不同的实体类型

​      (3) 命名实体存在嵌套现象，如“北京大学第三医院”这一组织机构名中还嵌套着同样可以作为组织机构名的“北京大学”，而且这种现象在组织机构名中尤其严重

​      (4) 中文里广泛存在简化表达现象，如“北医三院”、“国科大”，乃至简化表达构成的命名实体，如“国科大桥”。

​      专著 [1] 里比较详细地介绍了 NER 的各种方法（由于出版年限较早，未涵盖神经网络方法），这里笼统地摘取三类方法：

​      **1. 基于规则的方法**：利用手工编写的规则，将文本与规则进行匹配来识别出命名实体。例如，对于中文来说，“说”、“老师”等词语可作为人名的下文，“大学”、“医院”等词语可作为组织机构名的结尾，还可以利用到词性、句法信息。在构建规则的过程中往往需要大量的语言学知识，不同语言的识别规则不尽相同，而且需要谨慎处理规则之间的冲突问题；此外，构建规则的过程费时费力、可移植性不好。

​      **2. 基于特征模板的方法**： 

​      统计机器学习方法将 NER 视作序列标注任务，利用大规模语料来学习出标注模型，从而对句子的各个位置进行标注。常用的应用到 NER 任务中的模型包括生成式模型HMM、判别式模型CRF等。比较流行的方法是**特征模板 + CRF**的方案：特征模板通常是人工定义的一些二值特征函数，试图挖掘命名实体内部以及上下文的构成特点。对于句子中的给定位置来说，提特征的位置是一个窗口，即上下文位置。而且，不同的特征模板之间可以进行组合来形成一个新的特征模板。CRF的优点在于其为一个位置进行标注的过程中可以利用到此前已经标注的信息，利用Viterbi解码来得到最优序列。对句子中的各个位置提取特征时，满足条件的特征取值为1，不满足条件的特征取值为0；然后把特征喂给CRF，training阶段建模标签的转移，进而在inference阶段为测试句子的各个位置做标注。关于这种方法可以参阅文献 [2] 和 [3]。

​      **3. 基于神经网络的方法**：

​      近年来，随着硬件能力的发展以及词的分布式表示（word embedding）的出现，神经网络成为可以有效处理许多NLP任务的模型。这类方法对于序列标注任务（如CWS、POS、NER）的处理方式是类似的，将token从离散one-hot表示映射到低维空间中成为稠密的embedding，随后将句子的embedding序列输入到RNN中，用神经网络自动提取特征，Softmax来预测每个token的标签。这种方法使得模型的训练成为一个端到端的整体过程，而非传统的pipeline，不依赖特征工程，是一种数据驱动的方法；但网络变种多、对参数设置依赖大，模型可解释性差。此外，这种方法的一个缺点是对每个token打标签的过程中是独立的分类，不能直接利用上文已经预测的标签（只能靠隐状态传递上文信息），进而导致预测出的标签序列可能是非法的，例如标签B-PER后面是不可能紧跟着I-LOC的，但Softmax不会利用到这个信息。

​      学界提出了 LSTM-CRF 模型做序列标注。文献[4][5]在LSTM层后接入CRF层来做句子级别的标签预测，使得标注过程不再是对各个token独立分类。引入CRF这个idea最早其实可以追溯到文献[6]中。文献[5]还提出在英文NER任务中先使用LSTM来为每个单词由字母构造词并拼接到词向量后再输入到LSTM中，以捕捉单词的前后缀等字母形态特征。文献[8]将这个套路用在了中文NER任务中，用偏旁部首来构造汉字。关于神经网络方法做NER，可以看博客[9] ，介绍的非常详细～

### 标注方法

**BIO标注：**将每个元素标注为“B-X”、“I-X”或者“O”。其中，“B-X”表示此元素所在的片段属于X类型并且此元素在此片段的开头，“I-X”表示此元素所在的片段属于X类型并且此元素在此片段的中间位置，“O”表示不属于任何类型。

  比如，我们将 X 表示为名词短语（Noun Phrase, NP），则BIO的三个标记为：

（1）B-NP：名词短语的开头

（2）I-NP：名词短语的中间

（3）O：不是名词短语

**BIOSE标注：**

begin

intermediate

other

single

end



### **基于字的BiLSTM-CRF模型**

以句子为单位，将一个含有 n个字的句子（字的序列）记作
$$
x=(x_{1},x_{2},...,x_{n})
$$
其中 $x_{i}​$ 表示句子的第 $i​$ 个字在字典中的id，进而可以得到每个字的one-hot向量，维数是字典大小。

模型的第一层是 look-up 层，利用预训练或随机初始化的embedding矩阵将句子中的每个字 $x_{i}​$ 由one-hot向量映射为低维稠密的字向量（character embedding）$\boldsymbol x_{i}\in\mathbb R^{d}​$ ，$d​$ 是embedding的维度。在输入下一层之前，设置dropout以缓解过拟合。

模型的第二层是双向LSTM层，自动提取句子特征。将一个句子的各个字的char embedding序列 $(\boldsymbol x_{1},\boldsymbol x_{2},...,\boldsymbol x_{n})​$ 作为双向LSTM各个时间步的输入，再将正向LSTM输出的隐状态序列 $(\overset{\longrightarrow}{\boldsymbol h_1},\overset{\longrightarrow}{\boldsymbol h_2},...,\overset{\longrightarrow}{\boldsymbol h_n})​$ 与反向LSTM的 $(\overset{\longleftarrow}{\boldsymbol h_1},\overset{\longleftarrow}{\boldsymbol h_2},...,\overset{\longleftarrow}{\boldsymbol h_n})​$ 在各个位置输出的隐状态进行按位置拼接 $\boldsymbol h_{t}=[\overset{\longrightarrow}{\boldsymbol h_t};\overset{\longleftarrow}{\boldsymbol h_t}]\in\mathbb R^{m}​$ ，得到完整的隐状态序列

$$
({\boldsymbol h_1},{\boldsymbol h_2},...,{\boldsymbol h_n})\in\mathbb R^{n\times m}
$$
在设置dropout后，接入一个线性层，将隐状态向量从 $m​$ 维映射到 $k​$ 维，$k​$ 是标注集的标签数，从而得到自动提取的句子特征，记作矩阵 $P=({\boldsymbol p_1},{\boldsymbol p_2},...,{\boldsymbol p_n})\in\mathbb R^{n\times k}​$ 。可以把 $\boldsymbol p_i\in\mathbb R^{k}​$ 的每一维 $p_{ij}​$ 都视作将字 $x_{i}​$ 分类到第 $j​$ 个标签的打分值，如果再对 $P​$ 进行Softmax的话，就相当于对各个位置独立进行 $k​$ 类分类。但是这样对各个位置进行标注时无法利用已经标注过的信息，所以接下来将接入一个CRF层来进行标注。

模型的第三层是CRF层，进行句子级的序列标注。CRF层的参数是一个 $(k+2)\times (k+2)$ 的矩阵 $A$ ，$A_{ij}$ 表示的是从第 $i$ 个标签到第 $j$ 个标签的转移得分，进而在为一个位置进行标注的时候可以利用此前已经标注过的标签，之所以要加2是因为要为句子首部添加一个起始状态以及为句子尾部添加一个终止状态。如果记一个长度等于句子长度的标签序列 $y=(y_1,y_2,...,y_n)$ ，那么模型对于句子 $x$ 的标签等于 $y$ 的打分为
$$
score(x,y)=\sum_{i=1}^{n}P_{i,y_{i}}+\sum_{i=1}^{n+1}A_{y_{i-1},y_{i}}
$$
可以看出整个序列的打分等于各个位置的打分之和，而每个位置的打分由两部分得到，一部分是由LSTM输出的 $\boldsymbol p_i$ 决定，另一部分则由CRF的转移矩阵 $A$ 决定。进而可以利用Softmax得到归一化后的概率：
$$
P(y|x)=\frac{\exp(score(x,y))}{\sum_{y'}\exp(score(x,y'))}
$$
模型训练时通过最大化对数似然函数，下式给出了对一个训练样本 $(x,y^{x})$ 的对数似然：

$$
\log P(y^{x}|x)=score(x,y^{x})-\log(\sum_{y'}\exp(score(x,y')))

$$


如果这个算法要自己实现的话，需要注意的是指数的和的对数要转换成 $\log\sum_i\exp(x_i) = a + \log\sum_i\exp(x_i - a)​$（TensorFlow有现成的CRF，PyTorch就要自己写了），在CRF中上式的第二项使用前向后向算法来高效计算。

模型在预测过程（解码）时使用动态规划的Viterbi算法来求解最优路径：
$$
y^{*}=\arg\max_{y'}score(x,y')
$$
整个模型的结构如下图所示：

![](/assets/imgs/A06/NER.png)



## 参考文献

[^1 ]: https://www.cnblogs.com/Determined22/p/7238342.html “[序列标注：BiLSTM-CRF模型做基于字的中文命名实体识别](https://www.cnblogs.com/Determined22/p/7238342.html)”